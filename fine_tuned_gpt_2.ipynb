{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fine_tuned_gpt-2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq6V7KeGqJSR"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2nAch_YqTjr"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tphh_b3i2O63"
      },
      "source": [
        "TRAIN_FILE_PATH = \"/content/drive/My Drive/wikitext-2-raw/wiki.train.raw\"\n",
        "TEST_FILE_PATH = \"/content/drive/My Drive/wikitext-2-raw/wiki.test.raw\"\n",
        "\n",
        "text_train = open(TRAIN_FILE_PATH, 'r').read()\n",
        "text_test = open(TEST_FILE_PATH, 'r').read()\n",
        "\n",
        "with open(TRAIN_FILE_PATH + \".short\", \"w\") as f:\n",
        "  f.write(text_train[:1000000])\n",
        "\n",
        "with open(TEST_FILE_PATH + \".short\", \"w\") as f:\n",
        "  f.write(text_test[:500000])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CC8T2nLqWln"
      },
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHfEjmkF8dC5"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/language-modeling/run_language_modeling.py\n",
        "!ls -l *.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjTK5sOn9KXw"
      },
      "source": [
        "!python run_language_modeling.py \\\n",
        "    --output_dir=output \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path=gpt2 \\\n",
        "    --do_train \\\n",
        "    --train_data_file=$\"/content/drive/My Drive/wikitext-2-raw/wiki.train.raw.short\" \\\n",
        "    --do_eval \\\n",
        "    --eval_data_file=$\"/content/drive/My Drive/wikitext-2-raw/wiki.test.raw.short\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIyPTG9oqb7V"
      },
      "source": [
        "# Function to first select topN tokens from the probability list and then based on the selected N word distribution\n",
        "# get random token ID\n",
        "def choose_from_top(probs, n=5):\n",
        "    ind = np.argpartition(probs, -n)[-n:]\n",
        "    top_prob = probs[ind]\n",
        "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
        "    choice = np.random.choice(n, 1, p = top_prob)\n",
        "    token_id = ind[choice][0]\n",
        "    return int(token_id), top_prob[choice]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZH5pyEoqkc4"
      },
      "source": [
        "def generate_some_text(input_str, text_len = 100):\n",
        "    cur_ids = torch.LongTensor(tokenizer.encode(input_str)).to(device)\n",
        "    k=0\n",
        "    l=0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(text_len):\n",
        "            outputs = model(cur_ids, labels=cur_ids)\n",
        "            loss, logits = outputs[:2]\n",
        "            softmax_logits = torch.softmax(logits[-1], dim=0) #Take the first(only one) batch and the last predicted embedding\n",
        "            next_token_id, prob = choose_from_top(softmax_logits.cpu().numpy(), n=5) #Randomly(from the given probability distribution) choose the next word from the top n words\n",
        "            k+=1\n",
        "            l+=np.log2(prob)\n",
        "            if ([next_token_id] == tokenizer.encode(tokenizer.eos_token)): # if the network generated the end of the sentence, stop \n",
        "              break\n",
        "            cur_ids = torch.LongTensor(cur_ids.cpu().tolist() + [next_token_id]).to(device) # Add the last word \n",
        "\n",
        "        output_text = tokenizer.decode(cur_ids)\n",
        "        print(output_text)\n",
        "        print('perplexity=',np.power(2,-l/k))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukqbLTjpq0bu"
      },
      "source": [
        "generate_some_text(\"The rain was unexpectedly warm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oieeh-CH9mDq"
      },
      "source": [
        "def count_perplexity(encodings):\n",
        "  input_ids = encodings.input_ids.to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      outputs = model(input_ids, labels=input_ids)\n",
        "      loss=outputs[0]\n",
        "\n",
        "  ppl=math.exp(outputs[0])\n",
        "  return ppl"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSnw1jbT7znc",
        "outputId": "37a257cc-6789-4af1-8b50-5d4ab0999eba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "input=['The moon is made of chocolate.', 'The moon is made of cheese.', 'The moon is made of oxygen and silicon.', \n",
        "       'Lions live in forests and eat berries.', 'Lions live in cities and eat hoofed mammals.', 'Lions live in savannas and eat hoofed mammals.',\n",
        "       'All summer and autumn birds store fat to hibernate for the winte.', 'All summer and autumn bears store fat to hibernate for the winter.', \n",
        "       'Caterpillars run fast and see in the dark.', 'Cats run fast and see in the dark.',\n",
        "       'Dolphins are predators living in meadows.', 'Dolphins are predators living in seas.', 'Dolphins are predators living in the mountains.']\n",
        "for str in input:\n",
        "  tokens=tokenizer(str, return_tensors='pt')\n",
        "  result=count_perplexity(tokens)\n",
        "  print('perplexity of (', str, ') =', result)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "perplexity of ( The moon is made of chocolate. ) = 70.63049732650195\n",
            "perplexity of ( The moon is made of cheese. ) = 49.780604325072204\n",
            "perplexity of ( The moon is made of oxygen and silicon. ) = 42.888554838426735\n",
            "perplexity of ( Lions live in forests and eat berries. ) = 178.15567000019414\n",
            "perplexity of ( Lions live in cities and eat hoofed mammals. ) = 115.49040691303497\n",
            "perplexity of ( Lions live in savannas and eat hoofed mammals. ) = 56.169676999696016\n",
            "perplexity of ( All summer and autumn birds store fat to hibernate for the winte. ) = 185.25426346448492\n",
            "perplexity of ( All summer and autumn bears store fat to hibernate for the winter. ) = 67.57462327582572\n",
            "perplexity of ( Caterpillars run fast and see in the dark. ) = 34.48157288879732\n",
            "perplexity of ( Cats run fast and see in the dark. ) = 67.93385217505953\n",
            "perplexity of ( Dolphins are predators living in meadows. ) = 217.17167948787576\n",
            "perplexity of ( Dolphins are predators living in seas. ) = 304.0794363171705\n",
            "perplexity of ( Dolphins are predators living in the mountains. ) = 181.20860059172253\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
