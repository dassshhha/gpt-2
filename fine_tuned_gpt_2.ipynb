{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fine_tuned_gpt-2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq6V7KeGqJSR"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2nAch_YqTjr"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tphh_b3i2O63"
      },
      "source": [
        "TRAIN_FILE_PATH = \"/content/drive/My Drive/wikitext-2-raw/wiki.train.raw\"\n",
        "TEST_FILE_PATH = \"/content/drive/My Drive/wikitext-2-raw/wiki.test.raw\"\n",
        "\n",
        "text_train = open(TRAIN_FILE_PATH, 'r').read()\n",
        "text_test = open(TEST_FILE_PATH, 'r').read()\n",
        "\n",
        "with open(TRAIN_FILE_PATH + \".short\", \"w\") as f:\n",
        "  f.write(text_train[:1000000])\n",
        "\n",
        "with open(TEST_FILE_PATH + \".short\", \"w\") as f:\n",
        "  f.write(text_test[:500000])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CC8T2nLqWln"
      },
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHfEjmkF8dC5"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/language-modeling/run_language_modeling.py\n",
        "!ls -l *.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjTK5sOn9KXw",
        "outputId": "41a1e897-3e9b-4399-a615-2e83716ef631",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 776
        }
      },
      "source": [
        "!python run_language_modeling.py \\\n",
        "    --output_dir=output \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path=gpt2 \\\n",
        "    --do_train \\\n",
        "    --train_data_file=$\"/content/drive/My Drive/wikitext-2-raw/wiki.train.raw.short\" \\\n",
        "    --do_eval \\\n",
        "    --eval_data_file=$\"/content/drive/My Drive/wikitext-2-raw/wiki.test.raw.short\""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-16 10:15:33.222216: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/training_args.py:332: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n",
            "  FutureWarning,\n",
            "10/16/2020 10:15:34 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "10/16/2020 10:15:34 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='output', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Oct16_10-15-34_c96d4f7104be', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=False)\n",
            "10/16/2020 10:15:35 - INFO - filelock -   Lock 140370500130408 acquired on /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e.lock\n",
            "Downloading: 100% 665/665 [00:00<00:00, 665kB/s]\n",
            "10/16/2020 10:15:35 - INFO - filelock -   Lock 140370500130408 released on /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e.lock\n",
            "10/16/2020 10:15:36 - INFO - filelock -   Lock 140369974949872 acquired on /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71.lock\n",
            "Downloading: 100% 1.04M/1.04M [00:00<00:00, 2.82MB/s]\n",
            "10/16/2020 10:15:36 - INFO - filelock -   Lock 140369974949872 released on /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71.lock\n",
            "10/16/2020 10:15:37 - INFO - filelock -   Lock 140369974880688 acquired on /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 1.74MB/s]\n",
            "10/16/2020 10:15:37 - INFO - filelock -   Lock 140369974880688 released on /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/modeling_auto.py:785: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "10/16/2020 10:15:37 - INFO - filelock -   Lock 140370500130408 acquired on /root/.cache/torch/transformers/d71fd633e58263bd5e91dd3bde9f658bafd81e11ece622be6a3c2e4d42d8fd89.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1.lock\n",
            "Downloading: 100% 548M/548M [00:07<00:00, 77.4MB/s]\n",
            "10/16/2020 10:15:45 - INFO - filelock -   Lock 140370500130408 released on /root/.cache/torch/transformers/d71fd633e58263bd5e91dd3bde9f658bafd81e11ece622be6a3c2e4d42d8fd89.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1.lock\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1324: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
            "  FutureWarning,\n",
            "10/16/2020 10:15:50 - INFO - filelock -   Lock 140369952337704 acquired on /content/drive/My Drive/wikitext-2-raw/cached_lm_GPT2Tokenizer_1024_wiki.train.raw.short.lock\n",
            "10/16/2020 10:15:50 - INFO - filelock -   Lock 140369952337704 released on /content/drive/My Drive/wikitext-2-raw/cached_lm_GPT2Tokenizer_1024_wiki.train.raw.short.lock\n",
            "10/16/2020 10:15:50 - INFO - filelock -   Lock 140369952337760 acquired on /content/drive/My Drive/wikitext-2-raw/cached_lm_GPT2Tokenizer_1024_wiki.test.raw.short.lock\n",
            "10/16/2020 10:15:50 - INFO - filelock -   Lock 140369952337760 released on /content/drive/My Drive/wikitext-2-raw/cached_lm_GPT2Tokenizer_1024_wiki.test.raw.short.lock\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/trainer.py:267: FutureWarning: Passing `prediction_loss_only` as a keyword argument is deprecated and won't be possible in a future version. Use `args.prediction_loss_only` instead.\n",
            "  FutureWarning,\n",
            "Epoch:   0% 0/3 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration: 100% 1/1 [00:01<00:00,  1.15s/it]\n",
            "Epoch:  33% 1/3 [00:01<00:02,  1.15s/it]\n",
            "Iteration:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration: 100% 1/1 [00:00<00:00,  1.41it/s]\n",
            "Epoch:  67% 2/3 [00:01<00:01,  1.02s/it]\n",
            "Iteration:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration: 100% 1/1 [00:00<00:00,  1.33it/s]\n",
            "Epoch: 100% 3/3 [00:02<00:00,  1.15it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/trainer.py:1175: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n",
            "  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n",
            "10/16/2020 10:16:00 - INFO - __main__ -   *** Evaluate ***\n",
            "Evaluation: 100% 1/1 [00:00<00:00,  6.36it/s]\n",
            "{'eval_loss': 2.7315926551818848, 'epoch': 3.0, 'total_flos': 4587349082112, 'step': 3}\n",
            "10/16/2020 10:16:00 - INFO - __main__ -   ***** Eval results *****\n",
            "10/16/2020 10:16:00 - INFO - __main__ -     perplexity = 15.357326478521914\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIyPTG9oqb7V"
      },
      "source": [
        "# Function to first select topN tokens from the probability list and then based on the selected N word distribution\n",
        "# get random token ID\n",
        "def choose_from_top(probs, n=5):\n",
        "    ind = np.argpartition(probs, -n)[-n:]\n",
        "    top_prob = probs[ind]\n",
        "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
        "    choice = np.random.choice(n, 1, p = top_prob)\n",
        "    token_id = ind[choice][0]\n",
        "    return int(token_id), top_prob[choice]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZH5pyEoqkc4"
      },
      "source": [
        "def generate_some_text(input_str, text_len = 100):\n",
        "    cur_ids = torch.LongTensor(tokenizer.encode(input_str)).to(device)\n",
        "    k=0\n",
        "    l=0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(text_len):\n",
        "            outputs = model(cur_ids, labels=cur_ids)\n",
        "            loss, logits = outputs[:2]\n",
        "            softmax_logits = torch.softmax(logits[-1], dim=0) #Take the first(only one) batch and the last predicted embedding\n",
        "            next_token_id, prob = choose_from_top(softmax_logits.cpu().numpy(), n=5) #Randomly(from the given probability distribution) choose the next word from the top n words\n",
        "            k+=1\n",
        "            l+=np.log2(prob)\n",
        "            if ([next_token_id] == tokenizer.encode(tokenizer.eos_token)): # if the network generated the end of the sentence, stop \n",
        "              break\n",
        "            cur_ids = torch.LongTensor(cur_ids.cpu().tolist() + [next_token_id]).to(device) # Add the last word \n",
        "\n",
        "        output_text = tokenizer.decode(cur_ids)\n",
        "        print(output_text)\n",
        "        print('perplexity=',np.power(2,-l/k))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukqbLTjpq0bu"
      },
      "source": [
        "generate_some_text(\"The rain was unexpectedly warm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oieeh-CH9mDq"
      },
      "source": [
        "def count_perplexity(encodings):\n",
        "  input_ids = encodings.input_ids.to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      outputs = model(input_ids, labels=input_ids)\n",
        "      loss=outputs[0]\n",
        "\n",
        "  ppl=math.exp(outputs[0])\n",
        "  return ppl"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSnw1jbT7znc",
        "outputId": "8a1e903a-239f-4e31-e51f-baa5b85d68a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "input=['The moon is made of chocolate.', 'The moon is made of cheese.', 'The moon is made of oxygen and silicon.', \n",
        "       'Lions live in cities and eat berries.', 'Lions live in cities and eat hoofed mammals.', 'Lions live in savannas and eat hoofed mammals.',\n",
        "       'During the summer and autumn pigeons store fat to hibernate for the spring.','During the summer and autumn bears store fat to hibernate for the spring.', 'During the summer and autumn bears store fat to hibernate for the winter.', \n",
        "       'People see well in the dark.', 'Cats see well in the dark.',\n",
        "       'Kefir is made from bananas and meet.', 'Kefir is made from milk.',\n",
        "       'Spruce needles are red in winter and in summer they turns green.', 'Spruce needles are green both in winter and in summer.']\n",
        "for str in input:\n",
        "  tokens=tokenizer(str, return_tensors='pt')\n",
        "  result=count_perplexity(tokens)\n",
        "  print('perplexity of (', str, ') =', result)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "perplexity of ( The moon is made of chocolate. ) = 70.63049732650195\n",
            "perplexity of ( The moon is made of cheese. ) = 49.780604325072204\n",
            "perplexity of ( The moon is made of oxygen and silicon. ) = 42.888554838426735\n",
            "perplexity of ( Lions live in cities and eat berries. ) = 249.42906740815835\n",
            "perplexity of ( Lions live in cities and eat hoofed mammals. ) = 115.49040691303497\n",
            "perplexity of ( Lions live in savannas and eat hoofed mammals. ) = 56.169676999696016\n",
            "perplexity of ( During the summer and autumn pigeons store fat to hibernate for the spring. ) = 46.781344196156205\n",
            "perplexity of ( During the summer and autumn bears store fat to hibernate for the spring. ) = 53.68137365320195\n",
            "perplexity of ( During the summer and autumn bears store fat to hibernate for the winter. ) = 36.398219317308545\n",
            "perplexity of ( People see well in the dark. ) = 175.21189581355935\n",
            "perplexity of ( Cats see well in the dark. ) = 87.59922242092085\n",
            "perplexity of ( Kefir is made from bananas and meet. ) = 235.2903299159629\n",
            "perplexity of ( Kefir is made from milk. ) = 70.23121762626442\n",
            "perplexity of ( Spruce needles are red in winter and in summer they turns green. ) = 96.46644913545222\n",
            "perplexity of ( Spruce needles are green both in winter and in summer. ) = 55.4734443783285\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}