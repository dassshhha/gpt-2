{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fine_tuned_gpt-2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq6V7KeGqJSR"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2nAch_YqTjr"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tphh_b3i2O63"
      },
      "source": [
        "TRAIN_FILE_PATH = \"/content/drive/My Drive/wikitext-2-raw/wiki.train.raw\"\n",
        "TEST_FILE_PATH = \"/content/drive/My Drive/wikitext-2-raw/wiki.test.raw\"\n",
        "\n",
        "text_train = open(TRAIN_FILE_PATH, 'r').read()\n",
        "text_test = open(TEST_FILE_PATH, 'r').read()\n",
        "\n",
        "with open(TRAIN_FILE_PATH + \".short\", \"w\") as f:\n",
        "  f.write(text_train[:1000000])\n",
        "\n",
        "with open(TEST_FILE_PATH + \".short\", \"w\") as f:\n",
        "  f.write(text_test[:500000])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CC8T2nLqWln"
      },
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHfEjmkF8dC5"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/language-modeling/run_language_modeling.py\n",
        "!ls -l *.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjTK5sOn9KXw",
        "outputId": "c25cdf7b-2c33-43e0-aa20-6bcf5ad2957a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        }
      },
      "source": [
        "!python run_language_modeling.py \\\n",
        "    --output_dir=output \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path=gpt2 \\\n",
        "    --do_train \\\n",
        "    --train_data_file=$\"/content/drive/My Drive/wikitext-2-raw/wiki.train.raw.short\" \\\n",
        "    --do_eval \\\n",
        "    --eval_data_file=$\"/content/drive/My Drive/wikitext-2-raw/wiki.test.raw.short\""
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-06 20:43:35.755630: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/training_args.py:332: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n",
            "  FutureWarning,\n",
            "10/06/2020 20:43:37 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "10/06/2020 20:43:37 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='output', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Oct06_20-43-37_b93fd5d95fbb', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=False)\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/modeling_auto.py:785: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1324: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
            "  FutureWarning,\n",
            "10/06/2020 20:43:43 - INFO - filelock -   Lock 140247533423080 acquired on /content/drive/My Drive/wikitext-2-raw/cached_lm_GPT2Tokenizer_1024_wiki.train.raw.short.lock\n",
            "10/06/2020 20:43:43 - INFO - filelock -   Lock 140247533423080 released on /content/drive/My Drive/wikitext-2-raw/cached_lm_GPT2Tokenizer_1024_wiki.train.raw.short.lock\n",
            "10/06/2020 20:43:43 - INFO - filelock -   Lock 140247533422856 acquired on /content/drive/My Drive/wikitext-2-raw/cached_lm_GPT2Tokenizer_1024_wiki.test.raw.short.lock\n",
            "10/06/2020 20:43:43 - INFO - filelock -   Lock 140247533422856 released on /content/drive/My Drive/wikitext-2-raw/cached_lm_GPT2Tokenizer_1024_wiki.test.raw.short.lock\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/trainer.py:267: FutureWarning: Passing `prediction_loss_only` as a keyword argument is deprecated and won't be possible in a future version. Use `args.prediction_loss_only` instead.\n",
            "  FutureWarning,\n",
            "Epoch:   0% 0/3 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration: 100% 1/1 [00:00<00:00,  2.18it/s]\n",
            "Epoch:  33% 1/3 [00:00<00:00,  2.18it/s]\n",
            "Iteration:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration: 100% 1/1 [00:00<00:00,  2.49it/s]\n",
            "Epoch:  67% 2/3 [00:00<00:00,  2.26it/s]\n",
            "Iteration:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration: 100% 1/1 [00:00<00:00,  2.53it/s]\n",
            "Epoch: 100% 3/3 [00:01<00:00,  2.39it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/trainer.py:1175: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n",
            "  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n",
            "10/06/2020 20:43:50 - INFO - __main__ -   *** Evaluate ***\n",
            "Evaluation: 100% 1/1 [00:00<00:00, 15.32it/s]\n",
            "{'eval_loss': 2.7433931827545166, 'epoch': 3.0, 'total_flos': 4587349082112, 'step': 3}\n",
            "10/06/2020 20:43:51 - INFO - __main__ -   ***** Eval results *****\n",
            "10/06/2020 20:43:51 - INFO - __main__ -     perplexity = 15.539624524181512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIyPTG9oqb7V"
      },
      "source": [
        "# Function to first select topN tokens from the probability list and then based on the selected N word distribution\n",
        "# get random token ID\n",
        "def choose_from_top(probs, n=5):\n",
        "    ind = np.argpartition(probs, -n)[-n:]\n",
        "    top_prob = probs[ind]\n",
        "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
        "    choice = np.random.choice(n, 1, p = top_prob)\n",
        "    token_id = ind[choice][0]\n",
        "    return int(token_id), top_prob[choice]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZH5pyEoqkc4"
      },
      "source": [
        "def generate_some_text(input_str, text_len = 100):\n",
        "    cur_ids = torch.LongTensor(tokenizer.encode(input_str)).to(device)\n",
        "    k=0\n",
        "    l=0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(text_len):\n",
        "            outputs = model(cur_ids, labels=cur_ids)\n",
        "            loss, logits = outputs[:2]\n",
        "            softmax_logits = torch.softmax(logits[-1], dim=0) #Take the first(only one) batch and the last predicted embedding\n",
        "            next_token_id, prob = choose_from_top(softmax_logits.cpu().numpy(), n=5) #Randomly(from the given probability distribution) choose the next word from the top n words\n",
        "            k+=1\n",
        "            l+=np.log2(prob)\n",
        "            if ([next_token_id] == tokenizer.encode(tokenizer.eos_token)): # if the network generated the end of the sentence, stop \n",
        "              break\n",
        "            cur_ids = torch.LongTensor(cur_ids.cpu().tolist() + [next_token_id]).to(device)\n", # Add the last word
        "\n",
        "        output_text = tokenizer.decode(cur_ids)\n",
        "        print(output_text)\n",
        "        print('perplexity=',np.power(2,-l/k))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukqbLTjpq0bu",
        "outputId": "ab959b45-235e-4b04-9130-50e3d3cde4bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "generate_some_text(\"The rain was unexpectedly warm\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The rain was unexpectedly warm in the morning.\n",
            "\n",
            "The sun had set and the wind was blowing.\n",
            "\n",
            "\"It's not a bad time for a swim.\"\n",
            "\n",
            "The two of them were standing on the beach.\n",
            "\n",
            "The two of them looked at each other with a look of mutual respect.\n",
            "\n",
            "\"I think that it's better to go out and play. It's not like I'm in the mood.\"\n",
            "\n",
            "\"I don't mind if you don't go out and play.\"\n",
            "\n",
            "perplexity= [2.8187876]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
